{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üåç Tourism Data Collection Pipeline - Google Colab Setup\n",
                "\n",
                "This notebook sets up and runs the complete tourism data collection pipeline with AI enrichment.\n",
                "\n",
                "## Features:\n",
                "- ‚úÖ Gemma 3 12B model initialization\n",
                "- ‚úÖ OSM data extraction\n",
                "- ‚úÖ AI-powered POI enrichment (Price, Tips, Best Time)\n",
                "- ‚úÖ Destination profiling\n",
                "- ‚úÖ Supabase database loading\n",
                "\n",
                "## Requirements:\n",
                "- GPU runtime (T4 recommended)\n",
                "- Supabase credentials\n",
                "- Hugging Face Token (for Gemma model access)\n",
                "- ~2-3 hours for full pipeline"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üì¶ Step 1: Install Dependencies"
            ],
            "metadata": {
                "id": "step1"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "%%capture\n",
                "# Install required packages\n",
                "!pip install transformers accelerate bitsandbytes\n",
                "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
                "!pip install supabase requests overpy tqdm python-dotenv streamlit"
            ],
            "metadata": {
                "id": "install_deps"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üîê Step 2: Configure Credentials"
            ],
            "metadata": {
                "id": "step2"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "from google.colab import userdata\n",
                "\n",
                "# Set up Supabase credentials\n",
                "# Add these as Colab secrets: SUPABASE_URL, SUPABASE_KEY, HF_TOKEN\n",
                "try:\n",
                "    os.environ['SUPABASE_URL'] = userdata.get('SUPABASE_URL')\n",
                "    os.environ['SUPABASE_KEY'] = userdata.get('SUPABASE_KEY')\n",
                "    os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
                "    print(\"‚úÖ Credentials loaded from Colab secrets\")\n",
                "except:\n",
                "    print(\"‚ö†Ô∏è Credentials not found in secrets. Please add them manually:\")\n",
                "    os.environ['SUPABASE_URL'] = input(\"Enter SUPABASE_URL: \")\n",
                "    os.environ['SUPABASE_KEY'] = input(\"Enter SUPABASE_KEY: \")\n",
                "    os.environ['HF_TOKEN'] = input(\"Enter Hugging Face Token: \")\n",
                "    print(\"‚úÖ Credentials set\")\n",
                "\n",
                "# Login to Hugging Face\n",
                "from huggingface_hub import login\n",
                "login(token=os.environ['HF_TOKEN'])"
            ],
            "metadata": {
                "id": "credentials"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üì• Step 3: Clone Repository"
            ],
            "metadata": {
                "id": "step3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Clone the repository (or upload your code)\n",
                "!git clone https://github.com/alokanand1official/data-collector-be.git\n",
                "%cd data-collector-be/data_collector\n",
                "\n",
                "# Alternative: Upload from local\n",
                "# from google.colab import files\n",
                "# uploaded = files.upload()  # Upload your zipped codebase"
            ],
            "metadata": {
                "id": "clone_repo"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üíé Step 4: Initialize Gemma 3 Model"
            ],
            "metadata": {
                "id": "step4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "import json\n",
                "\n",
                "# Check GPU availability\n",
                "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
                "\n",
                "# Configure 4-bit quantization for memory efficiency\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16\n",
                ")\n",
                "\n",
                "# Load Gemma 3 12B model\n",
                "model_name = \"google/gemma-3-12b-it\"\n",
                "\n",
                "print(\"Loading tokenizer...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "print(\"Loading model (this may take 2-3 minutes)...\")\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Gemma 3 12B model loaded successfully!\")\n",
                "\n",
                "# Test the model\n",
                "def generate_text(prompt, max_length=512):\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=max_length,\n",
                "        temperature=0.7,\n",
                "        do_sample=True,\n",
                "        pad_token_id=tokenizer.eos_token_id\n",
                "    )\n",
                "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "# Test generation\n",
                "test_prompt = \"Generate a JSON object with a description of Paris: \"\n",
                "result = generate_text(test_prompt, max_length=100)\n",
                "print(\"\\nTest generation:\")\n",
                "print(result)"
            ],
            "metadata": {
                "id": "init_llama"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üîß Step 5: Create Gemma-based Enricher"
            ],
            "metadata": {
                "id": "step5"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "%%writefile etl/enrich/colab_ai_enricher.py\n",
                "import json\n",
                "import logging\n",
                "from pathlib import Path\n",
                "from typing import Dict, Any\n",
                "\n",
                "logger = logging.getLogger(\"ColabAIEnricher\")\n",
                "\n",
                "class ColabAIEnricher:\n",
                "    \"\"\"\n",
                "    AI Enricher using Hugging Face Transformers (for Colab).\n",
                "    Uses the loaded Gemma model directly instead of Ollama API.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, model, tokenizer, silver_dir: Path, gold_dir: Path):\n",
                "        self.model = model\n",
                "        self.tokenizer = tokenizer\n",
                "        self.silver_dir = silver_dir\n",
                "        self.gold_dir = gold_dir\n",
                "        \n",
                "    def process_city(self, city_name: str, limit: int = None) -> bool:\n",
                "        \"\"\"Enriches POIs for a city using the Gemma model.\"\"\"\n",
                "        city_key = city_name.lower().replace(\" \", \"_\")\n",
                "        \n",
                "        # Load Silver data\n",
                "        silver_file = self.silver_dir / city_key / \"pois.json\"\n",
                "        if not silver_file.exists():\n",
                "            logger.error(f\"Silver file not found: {silver_file}\")\n",
                "            return False\n",
                "            \n",
                "        with open(silver_file, 'r') as f:\n",
                "            pois = json.load(f)\n",
                "            \n",
                "        # Load existing Gold data\n",
                "        gold_file = self.gold_dir / city_key / \"pois.json\"\n",
                "        existing_pois = []\n",
                "        if gold_file.exists():\n",
                "            with open(gold_file, 'r') as f:\n",
                "                existing_pois = json.load(f)\n",
                "                \n",
                "        existing_ids = {poi.get('osm_id') for poi in existing_pois}\n",
                "        \n",
                "        # Filter POIs to enrich\n",
                "        to_enrich = [poi for poi in pois if poi.get('osm_id') not in existing_ids]\n",
                "        \n",
                "        if limit:\n",
                "            to_enrich = to_enrich[:limit]\n",
                "            \n",
                "        logger.info(f\"Enriching {len(to_enrich)} POIs for {city_name}...\")\n",
                "        \n",
                "        # Enrich POIs\n",
                "        enriched_pois = []\n",
                "        for i, poi in enumerate(to_enrich):\n",
                "            try:\n",
                "                enriched = self._enrich_poi(poi)\n",
                "                enriched_pois.append(enriched)\n",
                "                if (i + 1) % 10 == 0:\n",
                "                    logger.info(f\"Enriched {i + 1}/{len(to_enrich)} POIs\")\n",
                "            except Exception as e:\n",
                "                logger.error(f\"Failed to enrich {poi.get('name')}: {e}\")\n",
                "                enriched_pois.append(self._mock_enrich(poi))\n",
                "                \n",
                "        # Merge with existing\n",
                "        all_pois = existing_pois + enriched_pois\n",
                "        \n",
                "        # Save to Gold\n",
                "        gold_file.parent.mkdir(parents=True, exist_ok=True)\n",
                "        with open(gold_file, 'w') as f:\n",
                "            json.dump(all_pois, f, indent=2, ensure_ascii=False)\n",
                "            \n",
                "        logger.info(f\"‚úÖ Enriched {len(enriched_pois)} POIs. Total: {len(all_pois)}\")\n",
                "        return True\n",
                "        \n",
                "    def _enrich_poi(self, poi: Dict) -> Dict:\n",
                "        \"\"\"Enriches a single POI using the Gemma model.\"\"\"\n",
                "        prompt = f\"\"\"Analyze this tourism POI and provide enrichment data in JSON format:\n",
                "Name: {poi.get('name')}\n",
                "Type: {poi.get('poi_type')}\n",
                "Tags: {poi.get('tags')}\n",
                "\n",
                "Return ONLY valid JSON with:\n",
                "1. \"description\": Engaging 2-3 sentence description\n",
                "2. \"duration_min\": Recommended visit time (minutes)\n",
                "3. \"best_time\": Best time to visit (Morning/Afternoon/Evening/Anytime)\n",
                "4. \"best_time_reason\": Why this time is best\n",
                "5. \"price_level\": 0=Free, 1=Cheap, 2=Moderate, 3=Expensive\n",
                "6. \"tips\": Array of 2-3 practical tips\n",
                "7. \"what_to_expect\": One sentence summary\n",
                "8. \"personas\": Score 0-100 for {{Culture, Adventure, Food, Relax}}\n",
                "\"\"\"\n",
                "        \n",
                "        # Generate using Gemma\n",
                "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
                "        outputs = self.model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=512,\n",
                "            temperature=0.7,\n",
                "            do_sample=True,\n",
                "            pad_token_id=self.tokenizer.eos_token_id\n",
                "        )\n",
                "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        \n",
                "        # Extract JSON from response\n",
                "        try:\n",
                "            start = response.find('{')\n",
                "            end = response.rfind('}') + 1\n",
                "            if start != -1 and end > start:\n",
                "                enrichment = json.loads(response[start:end])\n",
                "                \n",
                "                # Merge enrichment with original POI\n",
                "                poi['description'] = enrichment.get('description', poi.get('description', ''))\n",
                "                poi['duration_min'] = enrichment.get('duration_min', 60)\n",
                "                poi['best_time'] = enrichment.get('best_time', 'Anytime')\n",
                "                poi['best_time_reason'] = enrichment.get('best_time_reason', 'Good time to visit')\n",
                "                poi['price_level'] = enrichment.get('price_level', 2)\n",
                "                poi['tips'] = enrichment.get('tips', [])\n",
                "                poi['what_to_expect'] = enrichment.get('what_to_expect', '')\n",
                "                poi['personas'] = enrichment.get('personas', {\"Culture\": 50, \"Relax\": 50})\n",
                "                poi['is_popular'] = enrichment.get('is_popular', False)\n",
                "                \n",
                "                return poi\n",
                "        except:\n",
                "            pass\n",
                "            \n",
                "        return self._mock_enrich(poi)\n",
                "            \n",
                "    def _mock_enrich(self, poi: Dict) -> Dict:\n",
                "        \"\"\"Fallback enrichment\"\"\"\n",
                "        poi['description'] = poi.get('description', f\"A wonderful place in {poi.get('city_name', 'the city')}.\")\n",
                "        poi['duration_min'] = 60\n",
                "        poi['best_time'] = \"Morning\"\n",
                "        poi['best_time_reason'] = \"Good lighting\"\n",
                "        poi['price_level'] = 2\n",
                "        poi['tips'] = [\"Check opening hours\"]\n",
                "        poi['what_to_expect'] = \"Interesting experience\"\n",
                "        poi['personas'] = {\"Culture\": 80, \"Relax\": 50}\n",
                "        return poi\n"
            ],
            "metadata": {
                "id": "create_enricher"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üèÉ Step 6: Run Bronze Layer (Data Extraction)"
            ],
            "metadata": {
                "id": "step6"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from orchestrator import Orchestrator\n",
                "\n",
                "# Initialize orchestrator\n",
                "orch = Orchestrator()\n",
                "\n",
                "# Choose cities to process\n",
                "# Azerbaijan Cities: Baku, Gabala, Sheki, Ganja, Quba, Lahij, Gobustan\n",
                "CITIES = [\"Baku\", \"Gabala\", \"Sheki\", \"Ganja\", \"Quba\", \"Lahij\", \"Gobustan\"]\n",
                "\n",
                "# Run Bronze Layer (OSM data extraction)\n",
                "for city in CITIES:\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"Extracting data for {city}...\")\n",
                "    print(f\"{'='*50}\")\n",
                "    orch.run_bronze_layer(city)\n",
                "    print(f\"‚úÖ Bronze layer complete for {city}\")"
            ],
            "metadata": {
                "id": "run_bronze"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üîÑ Step 7: Run Silver Layer (Data Transformation)"
            ],
            "metadata": {
                "id": "step7"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Run Silver Layer (data standardization)\n",
                "for city in CITIES:\n",
                "    print(f\"\\nTransforming data for {city}...\")\n",
                "    orch.run_silver_layer(city)\n",
                "    print(f\"‚úÖ Silver layer complete for {city}\")"
            ],
            "metadata": {
                "id": "run_silver"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ‚ú® Step 8: Run Gold Layer (AI Enrichment)"
            ],
            "metadata": {
                "id": "step8"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pathlib import Path\n",
                "from etl.enrich.colab_ai_enricher import ColabAIEnricher\n",
                "\n",
                "# Initialize Colab enricher with the loaded model\n",
                "enricher = ColabAIEnricher(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    silver_dir=Path(\"layers/silver\"),\n",
                "    gold_dir=Path(\"layers/gold\")\n",
                ")\n",
                "\n",
                "# Enrich POIs for each city\n",
                "for city in CITIES:\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"Enriching POIs for {city}...\")\n",
                "    print(f\"{'='*50}\")\n",
                "    enricher.process_city(city, limit=50)  # Limit to 50 POIs for faster testing\n",
                "    print(f\"‚úÖ Gold layer complete for {city}\")"
            ],
            "metadata": {
                "id": "run_gold"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üì§ Step 9: Load to Supabase"
            ],
            "metadata": {
                "id": "step9"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Load enriched data to Supabase\n",
                "for city in CITIES:\n",
                "    print(f\"\\nLoading {city} to Supabase...\")\n",
                "    orch.run_load_layer(city)\n",
                "    print(f\"‚úÖ Data loaded for {city}\")\n",
                "\n",
                "print(\"\\nüéâ Pipeline complete! All data loaded to Supabase.\")"
            ],
            "metadata": {
                "id": "run_load"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üìä Step 10: Verify Results"
            ],
            "metadata": {
                "id": "step10"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "# Check enriched data\n",
                "for city in CITIES:\n",
                "    city_key = city.lower().replace(\" \", \"_\")\n",
                "    gold_file = Path(f\"layers/gold/{city_key}/pois.json\")\n",
                "    \n",
                "    if gold_file.exists():\n",
                "        with open(gold_file, 'r') as f:\n",
                "            pois = json.load(f)\n",
                "        \n",
                "        print(f\"\\n{city}:\")\n",
                "        print(f\"  Total POIs: {len(pois)}\")\n",
                "        print(f\"  Sample POI:\")\n",
                "        if pois:\n",
                "            sample = pois[0]\n",
                "            print(f\"    Name: {sample.get('name')}\")\n",
                "            print(f\"    Description: {sample.get('description', 'N/A')[:100]}...\")\n",
                "            print(f\"    Duration: {sample.get('duration_min', 'N/A')} min\")\n",
                "            print(f\"    Best Time: {sample.get('best_time', 'N/A')}\")"
            ],
            "metadata": {
                "id": "verify"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üíæ Step 11: Download Results (Optional)"
            ],
            "metadata": {
                "id": "step11"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Zip and download the enriched data\n",
                "!zip -r enriched_data.zip layers/gold/\n",
                "\n",
                "from google.colab import files\n",
                "files.download('enriched_data.zip')\n",
                "\n",
                "print(\"‚úÖ Enriched data downloaded!\")"
            ],
            "metadata": {
                "id": "download"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}